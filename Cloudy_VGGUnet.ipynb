{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, absolute_import\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "# import data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# import image utils\n",
    "import PIL\n",
    "from PIL import Image, ImageOps, ImageEnhance\n",
    "\n",
    "# import image processing\n",
    "import scipy.ndimage as ndi\n",
    "import scipy\n",
    "\n",
    "# import image utilities\n",
    "from skimage.morphology import binary_opening, disk, label, binary_closing\n",
    "\n",
    "# import image augmentation\n",
    "from albumentations import (\n",
    "    HorizontalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,GaussianBlur,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, RandomBrightnessContrast, IAAPiecewiseAffine,\n",
    "    IAASharpen, IAAEmboss, Flip, OneOf, Compose, PadIfNeeded, RandomContrast, RandomGamma, RandomBrightness, ElasticTransform,\n",
    "    NoOp, RandomSizedCrop, RGBShift, VerticalFlip\n",
    ")\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, Sampler\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from torch.nn import init\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import math\n",
    "import os\n",
    "\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import cv2\n",
    "\n",
    "from skimage.morphology import binary_opening, disk, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = \"app.verta.ai\"\n",
    "\n",
    "PROJECT_NAME = \"Understanding_Clouds_from_Satellite_Images\"\n",
    "EXPERIMENT_NAME = \"VGGUNet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['VERTA_EMAIL'] = 'astakhova.aleksandra@gmail.com'\n",
    "os.environ['VERTA_DEV_KEY'] = 'd7ee32b5-bbd0-4c4c-a2ec-a070848021be'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set email from environment\n",
      "set developer key from environment\n",
      "connection successfully established\n",
      "set existing Project: Understanding_Clouds_from_Satellite_Images\n",
      "set existing Experiment: VGGUNet\n",
      "created new ExperimentRun: Run 1202015685863774967449\n"
     ]
    }
   ],
   "source": [
    "from verta import Client\n",
    "from verta.utils import ModelAPI\n",
    "\n",
    "client = Client(HOST)\n",
    "proj = client.set_project(PROJECT_NAME)\n",
    "expt = client.set_experiment(EXPERIMENT_NAME)\n",
    "run = client.set_experiment_run()\n",
    "\n",
    "run.log_tag('starter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = './train_images/'\n",
    "TEST_PATH = './test_images/'\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "#train_transforms = Compose([HorizontalFlip(p=0.5),\n",
    "#                            ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=30, p=0.2)])\n",
    "\n",
    "train_transforms = Compose([\n",
    "    OneOf([\n",
    "        ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1,\n",
    "                         rotate_limit=15,\n",
    "                         border_mode=cv2.BORDER_CONSTANT, value=0),\n",
    "        OpticalDistortion(distort_limit=0.11, shift_limit=0.15,\n",
    "                          border_mode=cv2.BORDER_CONSTANT,\n",
    "                          value=0),\n",
    "        NoOp()]),\n",
    "    #RandomSizedCrop(min_max_height=(1575, 1050),\n",
    "    #                height=1400,\n",
    "    #                width=1575, p=0.3),\n",
    "    OneOf([\n",
    "        RandomBrightnessContrast(brightness_limit=0.5,\n",
    "                                 contrast_limit=0.4),\n",
    "        RandomGamma(gamma_limit=(50, 150)),\n",
    "        NoOp()\n",
    "    ]),\n",
    "    OneOf([\n",
    "        RGBShift(r_shift_limit=20, b_shift_limit=15, g_shift_limit=15),\n",
    "        HueSaturationValue(hue_shift_limit=5,sat_shift_limit=5),\n",
    "        NoOp()\n",
    "    ]),\n",
    "    OneOf([\n",
    "        CLAHE(),\n",
    "        NoOp()\n",
    "    ]),\n",
    "    HorizontalFlip(p=0.5),\n",
    "    VerticalFlip(p=0.5)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image_Label</th>\n",
       "      <th>EncodedPixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0011165.jpg_Fish</td>\n",
       "      <td>264918 937 266318 937 267718 937 269118 937 27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0011165.jpg_Flower</td>\n",
       "      <td>1355565 1002 1356965 1002 1358365 1002 1359765...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0011165.jpg_Gravel</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0011165.jpg_Sugar</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>002be4f.jpg_Fish</td>\n",
       "      <td>233813 878 235213 878 236613 878 238010 881 23...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Image_Label                                      EncodedPixels\n",
       "0    0011165.jpg_Fish  264918 937 266318 937 267718 937 269118 937 27...\n",
       "1  0011165.jpg_Flower  1355565 1002 1356965 1002 1358365 1002 1359765...\n",
       "2  0011165.jpg_Gravel                                                NaN\n",
       "3   0011165.jpg_Sugar                                                NaN\n",
       "4    002be4f.jpg_Fish  233813 878 235213 878 236613 878 238010 881 23..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image_Label</th>\n",
       "      <th>EncodedPixels</th>\n",
       "      <th>Image</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0011165.jpg_Fish</td>\n",
       "      <td>264918 937 266318 937 267718 937 269118 937 27...</td>\n",
       "      <td>0011165.jpg</td>\n",
       "      <td>Fish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0011165.jpg_Flower</td>\n",
       "      <td>1355565 1002 1356965 1002 1358365 1002 1359765...</td>\n",
       "      <td>0011165.jpg</td>\n",
       "      <td>Flower</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0011165.jpg_Gravel</td>\n",
       "      <td>-1</td>\n",
       "      <td>0011165.jpg</td>\n",
       "      <td>Gravel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0011165.jpg_Sugar</td>\n",
       "      <td>-1</td>\n",
       "      <td>0011165.jpg</td>\n",
       "      <td>Sugar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>002be4f.jpg_Fish</td>\n",
       "      <td>233813 878 235213 878 236613 878 238010 881 23...</td>\n",
       "      <td>002be4f.jpg</td>\n",
       "      <td>Fish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Image_Label                                      EncodedPixels  \\\n",
       "0    0011165.jpg_Fish  264918 937 266318 937 267718 937 269118 937 27...   \n",
       "1  0011165.jpg_Flower  1355565 1002 1356965 1002 1358365 1002 1359765...   \n",
       "2  0011165.jpg_Gravel                                                 -1   \n",
       "3   0011165.jpg_Sugar                                                 -1   \n",
       "4    002be4f.jpg_Fish  233813 878 235213 878 236613 878 238010 881 23...   \n",
       "\n",
       "         Image   Label  \n",
       "0  0011165.jpg    Fish  \n",
       "1  0011165.jpg  Flower  \n",
       "2  0011165.jpg  Gravel  \n",
       "3  0011165.jpg   Sugar  \n",
       "4  002be4f.jpg    Fish  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split column\n",
    "split_df = train[\"Image_Label\"].str.split(\"_\", n = 1, expand = True)\n",
    "# add new columns to train_df\n",
    "train['Image'] = split_df[0]\n",
    "train['Label'] = split_df[1]\n",
    "# fiil missing masks with '-1'\n",
    "train = train.fillna('-1')\n",
    "\n",
    "# check the result\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_to_mask(rle_string, height, width):\n",
    "    rows, cols = height, width\n",
    "    \n",
    "    if rle_string == -1:\n",
    "        return np.zeros((height, width))\n",
    "    else:\n",
    "        rle_numbers = [int(num_string) for num_string in rle_string.split(' ')]\n",
    "        rle_pairs = np.array(rle_numbers).reshape(-1,2)\n",
    "        img = np.zeros(rows*cols, dtype=np.uint8)\n",
    "        for index, length in rle_pairs:\n",
    "            index -= 1\n",
    "            img[index:index+length] = 255\n",
    "        img = img.reshape(cols,rows)\n",
    "        img = img.T\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CloudDataset(Dataset):\n",
    "    def __init__(self, dataset_path, df, transforms=None, size = (256, 256), mode = 'train'):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.df = df\n",
    "        self.transforms = transforms\n",
    "        self.size = size\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # open image and label\n",
    "        id_code = self.df.loc[idx]['Image']\n",
    "        image = Image.open(os.path.join(self.dataset_path, str(id_code)))\n",
    "        \n",
    "        if self.mode == 'train' or self.mode == 'test':\n",
    "            rle_mask = self.df.loc[idx]['EncodedPixels']\n",
    "            if rle_mask == '-1':\n",
    "                np_mask = np.zeros(self.size)\n",
    "            else:\n",
    "                np_mask = rle_to_mask(rle_mask, image.size[1], image.size[0])\n",
    "        else:\n",
    "            np_mask = np.zeros(self.size)\n",
    "        \n",
    "        # augment image and mask\n",
    "        if self.transforms != None:\n",
    "            if (rle_mask) == '-1':\n",
    "                augmented = self.transforms(image=np.array(image))\n",
    "            else:\n",
    "                augmented = self.transforms(image=np.array(image), mask = np_mask)\n",
    "                np_mask = augmented['mask']\n",
    "            image = Image.fromarray(augmented['image'], 'RGB')\n",
    "        \n",
    "        # normalize and convert to tensor\n",
    "        tf = transforms.Compose([transforms.Resize(self.size),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "        image = tf(image)\n",
    "        \n",
    "        if (rle_mask) != \"-1\":\n",
    "            tf = transforms.Compose([transforms.Resize(self.size),\n",
    "                                 transforms.ToTensor()])\n",
    "            mask = Image.fromarray(np_mask, 'L')\n",
    "            mask = tf(mask).float()\n",
    "        else:\n",
    "            mask = TF.to_tensor(np_mask).float()\n",
    "        \n",
    "        # in validation mode return image and id_code\n",
    "        if self.mode == 'validation':\n",
    "            return image, id_code\n",
    "\n",
    "        # return tensor with image and label\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCEDiceLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BCEDiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        bce = F.binary_cross_entropy_with_logits(input, target)\n",
    "        smooth = 1e-5\n",
    "        input = torch.sigmoid(input)\n",
    "        num = target.size(0)\n",
    "        input = input.view(num, -1)\n",
    "        target = target.view(num, -1)\n",
    "        intersection = (input * target)\n",
    "        dice = (2. * intersection.sum(1) + smooth) / (input.sum(1) + target.sum(1) + smooth)\n",
    "        dice = 1 - dice.sum() / num\n",
    "        return 0.5 * bce + dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(output, target):\n",
    "    smooth = 1e-5\n",
    "\n",
    "    output = torch.sigmoid(output).view(-1).data.cpu().numpy()\n",
    "    target = target.view(-1).data.cpu().numpy()\n",
    "    intersection = (output * target).sum()\n",
    "\n",
    "    return (2. * intersection + smooth) / \\\n",
    "        (output.sum() + target.sum() + smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_score(output, target):\n",
    "    smooth = 1e-5\n",
    "\n",
    "    if torch.is_tensor(output):\n",
    "        output = torch.sigmoid(output).data.cpu().numpy()\n",
    "    if torch.is_tensor(target):\n",
    "        target = target.data.cpu().numpy()\n",
    "    output_ = output > 0.5\n",
    "    target_ = target > 0.5\n",
    "    intersection = (output_ & target_).sum()\n",
    "    union = (output_ | target_).sum()\n",
    "\n",
    "    return (intersection + smooth) / (union + smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Source:\n",
    "https://github.com/ternaus/TernausNet\n",
    "'''\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models\n",
    "\n",
    "def conv3x3(in_, out):\n",
    "    return nn.Conv2d(in_, out, 3, padding=1)\n",
    "\n",
    "\n",
    "class ConvRelu(nn.Module):\n",
    "    def __init__(self, in_, out):\n",
    "        super().__init__()\n",
    "        self.conv = conv3x3(in_, out)\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            ConvRelu(in_channels, middle_channels),\n",
    "            nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class UNet11(nn.Module):\n",
    "    def __init__(self, num_filters=32, pretrained=False):\n",
    "        \"\"\"\n",
    "        :param num_classes:\n",
    "        :param num_filters:\n",
    "        :param pretrained:\n",
    "            False - no pre-trained network is used\n",
    "            True  - encoder is pre-trained with VGG11\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.encoder = models.vgg11(pretrained=pretrained).features\n",
    "\n",
    "        self.relu = self.encoder[1]\n",
    "        self.conv1 = self.encoder[0]\n",
    "        self.conv2 = self.encoder[3]\n",
    "        self.conv3s = self.encoder[6]\n",
    "        self.conv3 = self.encoder[8]\n",
    "        self.conv4s = self.encoder[11]\n",
    "        self.conv4 = self.encoder[13]\n",
    "        self.conv5s = self.encoder[16]\n",
    "        self.conv5 = self.encoder[18]\n",
    "\n",
    "        self.center = DecoderBlock(num_filters * 8 * 2, num_filters * 8 * 2, num_filters * 8)\n",
    "        self.dec5 = DecoderBlock(num_filters * (16 + 8), num_filters * 8 * 2, num_filters * 8)\n",
    "        self.dec4 = DecoderBlock(num_filters * (16 + 8), num_filters * 8 * 2, num_filters * 4)\n",
    "        self.dec3 = DecoderBlock(num_filters * (8 + 4), num_filters * 4 * 2, num_filters * 2)\n",
    "        self.dec2 = DecoderBlock(num_filters * (4 + 2), num_filters * 2 * 2, num_filters)\n",
    "        self.dec1 = ConvRelu(num_filters * (2 + 1), num_filters)\n",
    "\n",
    "        self.final = nn.Conv2d(num_filters, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.relu(self.conv1(x))\n",
    "        conv2 = self.relu(self.conv2(self.pool(conv1)))\n",
    "        conv3s = self.relu(self.conv3s(self.pool(conv2)))\n",
    "        conv3 = self.relu(self.conv3(conv3s))\n",
    "        conv4s = self.relu(self.conv4s(self.pool(conv3)))\n",
    "        conv4 = self.relu(self.conv4(conv4s))\n",
    "        conv5s = self.relu(self.conv5s(self.pool(conv4)))\n",
    "        conv5 = self.relu(self.conv5(conv5s))\n",
    "\n",
    "        center = self.center(self.pool(conv5))\n",
    "\n",
    "        dec5 = self.dec5(torch.cat([center, conv5], 1))\n",
    "        dec4 = self.dec4(torch.cat([dec5, conv4], 1))\n",
    "        dec3 = self.dec3(torch.cat([dec4, conv3], 1))\n",
    "        dec2 = self.dec2(torch.cat([dec3, conv2], 1))\n",
    "        dec1 = self.dec1(torch.cat([dec2, conv1], 1))\n",
    "        return self.final(dec1)\n",
    "\n",
    "\n",
    "def unet11(pretrained=False, **kwargs):\n",
    "    \"\"\"\n",
    "    pretrained:\n",
    "            False - no pre-trained network is used\n",
    "            True  - encoder is pre-trained with VGG11\n",
    "            carvana - all weights are pre-trained on\n",
    "                Kaggle: Carvana dataset https://www.kaggle.com/c/carvana-image-masking-challenge\n",
    "    \"\"\"\n",
    "    model = UNet11(pretrained=pretrained, **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "class Interpolate(nn.Module):\n",
    "    def __init__(self, size=None, scale_factor=None, mode='nearest', align_corners=False):\n",
    "        super(Interpolate, self).__init__()\n",
    "        self.interp = nn.functional.interpolate\n",
    "        self.size = size\n",
    "        self.mode = mode\n",
    "        self.scale_factor = scale_factor\n",
    "        self.align_corners = align_corners\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.interp(x, size=self.size, scale_factor=self.scale_factor,\n",
    "                        mode=self.mode, align_corners=self.align_corners)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderBlockV2(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channels, out_channels, is_deconv=True):\n",
    "        super(DecoderBlockV2, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        if is_deconv:\n",
    "            \"\"\"\n",
    "                Paramaters for Deconvolution were chosen to avoid artifacts, following\n",
    "                link https://distill.pub/2016/deconv-checkerboard/\n",
    "            \"\"\"\n",
    "\n",
    "            self.block = nn.Sequential(\n",
    "                ConvRelu(in_channels, middle_channels),\n",
    "                nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=4, stride=2,\n",
    "                                   padding=1),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            self.block = nn.Sequential(\n",
    "                Interpolate(scale_factor=2, mode='bilinear'),\n",
    "                ConvRelu(in_channels, middle_channels),\n",
    "                ConvRelu(middle_channels, out_channels),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class AlbuNet(nn.Module):\n",
    "    \"\"\"\n",
    "        UNet (https://arxiv.org/abs/1505.04597) with Resnet34(https://arxiv.org/abs/1512.03385) encoder\n",
    "        Proposed by Alexander Buslaev: https://www.linkedin.com/in/al-buslaev/\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=1, num_filters=32, pretrained=False, is_deconv=False):\n",
    "        \"\"\"\n",
    "        :param num_classes:\n",
    "        :param num_filters:\n",
    "        :param pretrained:\n",
    "            False - no pre-trained network is used\n",
    "            True  - encoder is pre-trained with resnet34\n",
    "        :is_deconv:\n",
    "            False: bilinear interpolation is used in decoder\n",
    "            True: deconvolution is used in decoder\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.encoder = models.resnet34(pretrained=pretrained)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv1 = nn.Sequential(self.encoder.conv1,\n",
    "                                   self.encoder.bn1,\n",
    "                                   self.encoder.relu,\n",
    "                                   self.pool)\n",
    "\n",
    "        self.conv2 = self.encoder.layer1\n",
    "\n",
    "        self.conv3 = self.encoder.layer2\n",
    "\n",
    "        self.conv4 = self.encoder.layer3\n",
    "\n",
    "        self.conv5 = self.encoder.layer4\n",
    "\n",
    "        self.center = DecoderBlockV2(512, num_filters * 8 * 2, num_filters * 8, is_deconv)\n",
    "\n",
    "        self.dec5 = DecoderBlockV2(512 + num_filters * 8, num_filters * 8 * 2, num_filters * 8, is_deconv)\n",
    "        self.dec4 = DecoderBlockV2(256 + num_filters * 8, num_filters * 8 * 2, num_filters * 8, is_deconv)\n",
    "        self.dec3 = DecoderBlockV2(128 + num_filters * 8, num_filters * 4 * 2, num_filters * 2, is_deconv)\n",
    "        self.dec2 = DecoderBlockV2(64 + num_filters * 2, num_filters * 2 * 2, num_filters * 2 * 2, is_deconv)\n",
    "        self.dec1 = DecoderBlockV2(num_filters * 2 * 2, num_filters * 2 * 2, num_filters, is_deconv)\n",
    "        self.dec0 = ConvRelu(num_filters, num_filters)\n",
    "        self.final = nn.Conv2d(num_filters, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.conv1(x)\n",
    "        conv2 = self.conv2(conv1)\n",
    "        conv3 = self.conv3(conv2)\n",
    "        conv4 = self.conv4(conv3)\n",
    "        conv5 = self.conv5(conv4)\n",
    "\n",
    "        center = self.center(self.pool(conv5))\n",
    "\n",
    "        dec5 = self.dec5(torch.cat([center, conv5], 1))\n",
    "\n",
    "        dec4 = self.dec4(torch.cat([dec5, conv4], 1))\n",
    "        dec3 = self.dec3(torch.cat([dec4, conv3], 1))\n",
    "        dec2 = self.dec2(torch.cat([dec3, conv2], 1))\n",
    "        dec1 = self.dec1(dec2)\n",
    "        dec0 = self.dec0(dec1)\n",
    "\n",
    "        if self.num_classes > 1:\n",
    "            x_out = F.log_softmax(self.final(dec0), dim=1)\n",
    "        else:\n",
    "            x_out = self.final(dec0)\n",
    "\n",
    "        return x_out\n",
    "\n",
    "\n",
    "class UNet16(nn.Module):\n",
    "    def __init__(self, num_classes=1, num_filters=32, pretrained=False, is_deconv=False):\n",
    "        \"\"\"\n",
    "        :param num_classes:\n",
    "        :param num_filters:\n",
    "        :param pretrained:\n",
    "            False - no pre-trained network used\n",
    "            True - encoder pre-trained with VGG16\n",
    "        :is_deconv:\n",
    "            False: bilinear interpolation is used in decoder\n",
    "            True: deconvolution is used in decoder\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.encoder = models.vgg16(pretrained=pretrained).features\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv1 = nn.Sequential(self.encoder[0],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[2],\n",
    "                                   self.relu)\n",
    "\n",
    "        self.conv2 = nn.Sequential(self.encoder[5],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[7],\n",
    "                                   self.relu)\n",
    "\n",
    "        self.conv3 = nn.Sequential(self.encoder[10],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[12],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[14],\n",
    "                                   self.relu)\n",
    "\n",
    "        self.conv4 = nn.Sequential(self.encoder[17],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[19],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[21],\n",
    "                                   self.relu)\n",
    "\n",
    "        self.conv5 = nn.Sequential(self.encoder[24],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[26],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[28],\n",
    "                                   self.relu)\n",
    "\n",
    "        self.center = DecoderBlockV2(512, num_filters * 8 * 2, num_filters * 8, is_deconv)\n",
    "\n",
    "        self.dec5 = DecoderBlockV2(512 + num_filters * 8, num_filters * 8 * 2, num_filters * 8, is_deconv)\n",
    "        self.dec4 = DecoderBlockV2(512 + num_filters * 8, num_filters * 8 * 2, num_filters * 8, is_deconv)\n",
    "        self.dec3 = DecoderBlockV2(256 + num_filters * 8, num_filters * 4 * 2, num_filters * 2, is_deconv)\n",
    "        self.dec2 = DecoderBlockV2(128 + num_filters * 2, num_filters * 2 * 2, num_filters, is_deconv)\n",
    "        self.dec1 = ConvRelu(64 + num_filters, num_filters)\n",
    "        self.final = nn.Conv2d(num_filters, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.conv1(x)\n",
    "        conv2 = self.conv2(self.pool(conv1))\n",
    "        conv3 = self.conv3(self.pool(conv2))\n",
    "        conv4 = self.conv4(self.pool(conv3))\n",
    "        conv5 = self.conv5(self.pool(conv4))\n",
    "\n",
    "        center = self.center(self.pool(conv5))\n",
    "\n",
    "        dec5 = self.dec5(torch.cat([center, conv5], 1))\n",
    "\n",
    "        dec4 = self.dec4(torch.cat([dec5, conv4], 1))\n",
    "        dec3 = self.dec3(torch.cat([dec4, conv3], 1))\n",
    "        dec2 = self.dec2(torch.cat([dec3, conv2], 1))\n",
    "        dec1 = self.dec1(torch.cat([dec2, conv1], 1))\n",
    "\n",
    "        if self.num_classes > 1:\n",
    "            x_out = F.log_softmax(self.final(dec1), dim=1)\n",
    "        else:\n",
    "            x_out = self.final(dec1)\n",
    "\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet16(pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "test_split = 0.2\n",
    "batch_size = 16\n",
    "epochs = 40\n",
    "learning_rate = 0.0001\n",
    "num_workers = 1\n",
    "image_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "hyperparameter with key test_split already exists; consider using observations instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-2eb5efc26a94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_hyperparameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test_split\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_hyperparameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"batch_size\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_hyperparameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epochs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_hyperparameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"learning_rate\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_hyperparameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"image_size\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/verta/client.py\u001b[0m in \u001b[0;36mlog_hyperparameter\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2065\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m409\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2066\u001b[0m                 raise ValueError(\"hyperparameter with key {} already exists;\"\n\u001b[0;32m-> 2067\u001b[0;31m                                  \" consider using observations instead\".format(key))\n\u001b[0m\u001b[1;32m   2068\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2069\u001b[0m                 \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_http_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: hyperparameter with key test_split already exists; consider using observations instead"
     ]
    }
   ],
   "source": [
    "run.log_hyperparameter(\"test_split\", test_split)\n",
    "run.log_hyperparameter(\"batch_size\", batch_size)\n",
    "run.log_hyperparameter(\"epochs\", epochs)\n",
    "run.log_hyperparameter(\"learning_rate\", learning_rate)\n",
    "run.log_hyperparameter(\"image_size\", image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset and data loaders (for FISH)\n",
    "df = train[train['Label'] == 'Fish']\n",
    "df = df.set_index(pd.Series(range(len(df))))\n",
    "\n",
    "train_ds = CloudDataset(TRAIN_PATH, df, \n",
    "                        transforms=train_transforms, size = (image_size, image_size), mode = 'train')\n",
    "test_ds = CloudDataset(TRAIN_PATH, df, \n",
    "                       transforms=None, size = (image_size, image_size), mode = 'test')\n",
    "\n",
    "dataset_size = len(train_ds)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(test_split * dataset_size))\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "train_indices, test_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\n",
    "testloader = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, sampler=test_sampler, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "\n",
    "class RAdam(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        self.buffer = [[None, None, None] for ind in range(10)]\n",
    "        super(RAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "\n",
    "                state['step'] += 1\n",
    "                buffered = self.buffer[int(state['step'] % 10)]\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "\n",
    "                    # more conservative since it's an approximated value\n",
    "                    if N_sma >= 5:\n",
    "                        step_size = group['lr'] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = group['lr'] / (1 - beta1 ** state['step'])\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                if N_sma >= 5:            \n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n",
    "                else:\n",
    "                    p_data_fp32.add_(-step_size, exp_avg)\n",
    "\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = RAdam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_stats = pd.DataFrame(columns = ['Epoch', \n",
    "                                      'Time per epoch', \n",
    "                                      'Avg time per step', \n",
    "                                      'Train loss',\n",
    "                                      'DICE Train',\n",
    "                                      'Test loss',\n",
    "                                     'DICE Test']) \n",
    "criterion = BCEDiceLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "hyperparameter with key optimizer already exists; consider using observations instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-4ea84609d9c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_hyperparameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"optimizer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"RAdam\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_hyperparameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"BCEDiceLoss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/verta/client.py\u001b[0m in \u001b[0;36mlog_hyperparameter\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2065\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m409\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2066\u001b[0m                 raise ValueError(\"hyperparameter with key {} already exists;\"\n\u001b[0;32m-> 2067\u001b[0;31m                                  \" consider using observations instead\".format(key))\n\u001b[0m\u001b[1;32m   2068\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2069\u001b[0m                 \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_http_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: hyperparameter with key optimizer already exists; consider using observations instead"
     ]
    }
   ],
   "source": [
    "run.log_hyperparameter(\"optimizer\", \"RAdam\")\n",
    "run.log_hyperparameter(\"loss\", \"BCEDiceLoss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, device, trainloader, testloader, epochs, criterion, optimizer, train_stats):\n",
    "    #train the model\n",
    "    model.to(device)\n",
    "    \n",
    "    # learning rate cosine annealing\n",
    "    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, len(trainloader), eta_min=0.000001)\n",
    "\n",
    "    steps = 0\n",
    "    running_loss = 0\n",
    "    dice_train = 0\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        since = time.time()\n",
    "\n",
    "        train_accuracy = 0\n",
    "        dice_train = 0\n",
    "\n",
    "        \n",
    "        for inputs, labels in tqdm_notebook(trainloader):\n",
    "            steps += 1\n",
    "            # Move input and label tensors to the default device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logps = model.forward(inputs)\n",
    "            \n",
    "            loss = criterion(logps, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            dice_train += dice_coef(logps, labels)\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "\n",
    "        test_loss = 0\n",
    "        dice_test = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in testloader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                logps = model.forward(inputs)\n",
    "                batch_loss = criterion(logps, labels)\n",
    "\n",
    "                test_loss += batch_loss.item()\n",
    "\n",
    "                # Calculate DICE\n",
    "                dice_test += dice_coef(logps, labels)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "              f\"Time per epoch: {time_elapsed:.4f}.. \"\n",
    "              f\"Average time per step: {time_elapsed/len(trainloader):.4f}.. \"\n",
    "              f\"Train loss: {running_loss/len(trainloader):.4f}.. \"\n",
    "              f\"Test loss: {test_loss/len(testloader):.4f}.. \"\n",
    "              f\"DICE Train: {dice_train/len(trainloader):.4f}.. \"\n",
    "              f\"DICE Test: {dice_test/len(testloader):.4f}.. \"\n",
    "             )\n",
    "\n",
    "        train_stats = train_stats.append({'Epoch': epoch + 1, \n",
    "                                          'Time per epoch':time_elapsed, \n",
    "                                          'Avg time per step': time_elapsed/len(trainloader), \n",
    "                                          'Train loss' : running_loss/len(trainloader),\n",
    "                                          'Test loss' : test_loss/len(testloader),\n",
    "                                          'DICE Train' : dice_train/len(trainloader),\n",
    "                                          'DICE Test' : dice_test/len(testloader),\n",
    "                                         }, \n",
    "                                         ignore_index=True)\n",
    "\n",
    "        run.log_observation(\"time_per_epoch\", time_elapsed)\n",
    "        run.log_observation(\"time_per_step\", time_elapsed/len(trainloader))\n",
    "        run.log_observation(\"train_loss\", running_loss/len(trainloader))\n",
    "        run.log_observation(\"test_loss\", test_loss/len(testloader))\n",
    "        run.log_observation(\"dice_train\", dice_train/len(trainloader))\n",
    "        run.log_observation(\"dice_test\", dice_test/len(testloader))\n",
    "\n",
    "              \n",
    "        running_loss = 0\n",
    "        model.train()\n",
    "        scheduler.step()\n",
    "        \n",
    "    return model, train_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab58bd5234c94baca41102f0b1b1f664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40.. Time per epoch: 360.8192.. Average time per step: 1.2979.. Train loss: 1.0412.. Test loss: 0.9383.. DICE Train: 0.2796.. DICE Test: 0.4208.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e2bb39c812420ea7d54db85511f0ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/40.. Time per epoch: 414.1417.. Average time per step: 1.4897.. Train loss: 0.9553.. Test loss: 0.9116.. DICE Train: 0.3886.. DICE Test: 0.4411.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1347b7d955447e18c76773557048d3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/40.. Time per epoch: 408.0755.. Average time per step: 1.4679.. Train loss: 0.9337.. Test loss: 0.9599.. DICE Train: 0.4158.. DICE Test: 0.4232.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2188c52945e49278c2b44f94e10eae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/40.. Time per epoch: 409.9639.. Average time per step: 1.4747.. Train loss: 0.9256.. Test loss: 0.9423.. DICE Train: 0.4283.. DICE Test: 0.4457.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be89ad8147d540d7b5ca0a074d8ef39a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/40.. Time per epoch: 406.7518.. Average time per step: 1.4631.. Train loss: 0.9223.. Test loss: 0.9286.. DICE Train: 0.4323.. DICE Test: 0.4366.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a07e9f97dff49249d146b2f3de64233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/40.. Time per epoch: 407.6174.. Average time per step: 1.4662.. Train loss: 0.9176.. Test loss: 0.9151.. DICE Train: 0.4382.. DICE Test: 0.4282.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77ba587b34724adc9371419dbc8225af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/40.. Time per epoch: 411.0618.. Average time per step: 1.4786.. Train loss: 0.9132.. Test loss: 0.9077.. DICE Train: 0.4446.. DICE Test: 0.4381.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab76685e05eb4b1e9a9f35c450e53fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/40.. Time per epoch: 415.1828.. Average time per step: 1.4935.. Train loss: 0.9143.. Test loss: 0.9059.. DICE Train: 0.4426.. DICE Test: 0.4517.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a7bf88787af48b8a22f18992d2aa453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/40.. Time per epoch: 413.9601.. Average time per step: 1.4891.. Train loss: 0.9010.. Test loss: 0.9015.. DICE Train: 0.4635.. DICE Test: 0.4613.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f54d405bab2d4804a0bb1e4ab23c4cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/40.. Time per epoch: 405.3221.. Average time per step: 1.4580.. Train loss: 0.9011.. Test loss: 0.9270.. DICE Train: 0.4650.. DICE Test: 0.4507.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd6d4d3d40464e3b8de40bf02439e4da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/40.. Time per epoch: 410.0585.. Average time per step: 1.4750.. Train loss: 0.8993.. Test loss: 0.9017.. DICE Train: 0.4642.. DICE Test: 0.4837.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd90c728eac24da780cfeba3827911e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/40.. Time per epoch: 417.0451.. Average time per step: 1.5002.. Train loss: 0.8949.. Test loss: 0.9366.. DICE Train: 0.4721.. DICE Test: 0.4622.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca55c02bbf04c67be742e4baf15c0fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/40.. Time per epoch: 418.2189.. Average time per step: 1.5044.. Train loss: 0.8919.. Test loss: 0.9213.. DICE Train: 0.4725.. DICE Test: 0.4536.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "139f93ce4449498eb8f32cf807687287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/40.. Time per epoch: 401.1932.. Average time per step: 1.4431.. Train loss: 0.8852.. Test loss: 0.9239.. DICE Train: 0.4802.. DICE Test: 0.4544.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3536408b9ec4b9da2bb882973b62ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/40.. Time per epoch: 421.4213.. Average time per step: 1.5159.. Train loss: 0.8763.. Test loss: 0.9030.. DICE Train: 0.4980.. DICE Test: 0.4699.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce58a7e6c98d47d3956806997d971f40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/40.. Time per epoch: 409.2085.. Average time per step: 1.4720.. Train loss: 0.8718.. Test loss: 0.9066.. DICE Train: 0.5026.. DICE Test: 0.4698.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a64d57cfdac749dba588957bb8589a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/40.. Time per epoch: 407.4191.. Average time per step: 1.4655.. Train loss: 0.8687.. Test loss: 0.9030.. DICE Train: 0.5079.. DICE Test: 0.4992.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48995f254e704cd3a9e6734e6bbab888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/40.. Time per epoch: 405.5917.. Average time per step: 1.4590.. Train loss: 0.8615.. Test loss: 0.9134.. DICE Train: 0.5156.. DICE Test: 0.4865.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e59a82bc5524ec185007752211f8948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/40.. Time per epoch: 411.3432.. Average time per step: 1.4797.. Train loss: 0.8531.. Test loss: 0.9353.. DICE Train: 0.5288.. DICE Test: 0.4755.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c42edf455c4ed6bce9394fb1bde8b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/40.. Time per epoch: 408.1714.. Average time per step: 1.4682.. Train loss: 0.8476.. Test loss: 0.9304.. DICE Train: 0.5386.. DICE Test: 0.4945.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8c69067a4f440f5a736412b3226f8f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/40.. Time per epoch: 405.4822.. Average time per step: 1.4586.. Train loss: 0.8407.. Test loss: 0.9504.. DICE Train: 0.5509.. DICE Test: 0.4597.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b019b550ae284feb82c2cacd358617b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/40.. Time per epoch: 417.1099.. Average time per step: 1.5004.. Train loss: 0.8424.. Test loss: 0.9199.. DICE Train: 0.5430.. DICE Test: 0.4799.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ac0889ffd4a4837b9a075817b84226a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/40.. Time per epoch: 412.6293.. Average time per step: 1.4843.. Train loss: 0.8294.. Test loss: 0.9159.. DICE Train: 0.5653.. DICE Test: 0.4888.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "482b1c83e866444fb4aa2b2dc76edbb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/40.. Time per epoch: 412.9710.. Average time per step: 1.4855.. Train loss: 0.8284.. Test loss: 0.9316.. DICE Train: 0.5637.. DICE Test: 0.4944.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5de9ea03a30e40f086e822add27a096f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/40.. Time per epoch: 407.6556.. Average time per step: 1.4664.. Train loss: 0.8095.. Test loss: 0.9171.. DICE Train: 0.5936.. DICE Test: 0.4801.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc9e0bd93db94b7a96f0ba3dd5438dd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/40.. Time per epoch: 408.9680.. Average time per step: 1.4711.. Train loss: 0.8104.. Test loss: 0.9369.. DICE Train: 0.5928.. DICE Test: 0.4840.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3449a47a01c47a487b197ee8da1b7ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/40.. Time per epoch: 407.4697.. Average time per step: 1.4657.. Train loss: 0.7982.. Test loss: 0.9691.. DICE Train: 0.6136.. DICE Test: 0.4806.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f31c2a7a4bc54b9588a6fc57643e5d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/40.. Time per epoch: 405.3817.. Average time per step: 1.4582.. Train loss: 0.7914.. Test loss: 0.9449.. DICE Train: 0.6207.. DICE Test: 0.4704.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd13cace203481da8f99a6dc2d809ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/40.. Time per epoch: 408.9555.. Average time per step: 1.4711.. Train loss: 0.7842.. Test loss: 0.9669.. DICE Train: 0.6345.. DICE Test: 0.4424.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b1e9be511f4b7081d8191722db40c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/40.. Time per epoch: 410.2214.. Average time per step: 1.4756.. Train loss: 0.7821.. Test loss: 0.9568.. DICE Train: 0.6393.. DICE Test: 0.4771.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a546343199c24616a2bb34c65de268b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/40.. Time per epoch: 405.4413.. Average time per step: 1.4584.. Train loss: 0.7752.. Test loss: 0.9416.. DICE Train: 0.6482.. DICE Test: 0.4939.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb722a34ebc944fe85b36eadab08da6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/40.. Time per epoch: 413.8253.. Average time per step: 1.4886.. Train loss: 0.7671.. Test loss: 0.9648.. DICE Train: 0.6596.. DICE Test: 0.4809.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f9b5de7c054782baf016380a669fbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/40.. Time per epoch: 411.0461.. Average time per step: 1.4786.. Train loss: 0.7594.. Test loss: 0.9968.. DICE Train: 0.6702.. DICE Test: 0.4481.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc150141f23749e688e4373d52e28733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/40.. Time per epoch: 405.8863.. Average time per step: 1.4600.. Train loss: 0.7636.. Test loss: 0.9612.. DICE Train: 0.6627.. DICE Test: 0.4978.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c078f8906e314ad9b21192f5b813bea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/40.. Time per epoch: 413.5508.. Average time per step: 1.4876.. Train loss: 0.7488.. Test loss: 0.9756.. DICE Train: 0.6882.. DICE Test: 0.4590.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f0defe042e6453ab28c01ea7f2ead63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train the model\n",
    "model, train_stats = train_model(model, device, trainloader, testloader, epochs, criterion, optimizer, train_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'vggunet_' + str(epochs) + '.pth'\n",
    "\n",
    "checkpoint = {'state_dict': model.state_dict()}\n",
    "\n",
    "torch.save(checkpoint, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save training stats\n",
    "train_stats.to_csv('vggunet_stats.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_dataset('train_stats', train_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_dataset('model', checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([obs[0] for obs in run.get_observation(\"test_loss\")], label=\"test\")\n",
    "plt.plot([obs[0] for obs in run.get_observation(\"train_loss\")], label=\"train\")\n",
    "plt.ylim(0, 1.4)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend(loc='best')\n",
    "run.log_image(\"loss\", plt)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([obs[0] for obs in run.get_observation(\"dice_test\")], label=\"test\")\n",
    "plt.plot([obs[0] for obs in run.get_observation(\"dice_train\")], label=\"train\")\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"dice\")\n",
    "plt.legend(loc='best')\n",
    "run.log_image(\"dice\", plt)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = 0\n",
    "dice_test = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in testloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logps = model.forward(inputs)\n",
    "        batch_loss = criterion(logps, labels)\n",
    "\n",
    "        test_loss += batch_loss.item()\n",
    "\n",
    "        # Calculate DICE\n",
    "        dice_test += dice_coef(logps, labels)\n",
    "        \n",
    "test_loss = test_loss / len(testloader)\n",
    "dice_test = dice_test / len(testloader)\n",
    "\n",
    "run.log_metric(\"test_loss\", test_loss)\n",
    "run.log_metric(\"dice_test\", dice_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask2rle(img):\n",
    "    '''\n",
    "    Convert mask to rle.\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels= img.T.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_width = image_size\n",
    "im_height = image_size\n",
    "\n",
    "orig_width = 2100\n",
    "orig_height = 1400\n",
    "\n",
    "threshold = 0.3\n",
    "\n",
    "n_images = 5\n",
    "fig, ax = plt.subplots(n_images, 3, figsize=(10, 10))\n",
    "\n",
    "indices = [np.random.choice(test_indices) for i in range(n_images)]\n",
    "\n",
    "model.eval()\n",
    "c = 0\n",
    "with torch.no_grad():\n",
    "    for idx in indices:\n",
    "        \n",
    "        id_code = df.loc[idx]['Image']\n",
    "        image = Image.open(os.path.join(TRAIN_PATH, str(id_code)))\n",
    "    \n",
    "        tf = transforms.Compose([transforms.Resize((im_width,im_height)),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "        inputs = tf(image)\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        output = model.forward(inputs.reshape(1,3,image_size,image_size))\n",
    "\n",
    "        pred_mask = torch.sigmoid(output.reshape(im_width,im_height)).data.cpu().numpy()\n",
    "        pred_mask = binary_opening(pred_mask > threshold, disk(2))\n",
    "\n",
    "        pred_mask = Image.fromarray((pred_mask * 255).astype(np.uint8)).resize((orig_width,orig_height))\n",
    "        \n",
    "        rle_mask = df.loc[idx]['EncodedPixels']\n",
    "        if rle_mask == '-1':\n",
    "            np_mask = np.zeros((orig_height, orig_width))\n",
    "        else:\n",
    "            np_mask = rle_to_mask(rle_mask, orig_height, orig_width)\n",
    "            \n",
    "        # plot images and masks\n",
    "        ax[c, 0].imshow(image)\n",
    "        ax[c, 0].axis('off')\n",
    "        ax[c, 0].set_title('Image')\n",
    "        \n",
    "        # plot images and masks\n",
    "        ax[c, 1].imshow(np_mask)\n",
    "        ax[c, 1].axis('off')\n",
    "        ax[c, 1].set_title('Original Mask')\n",
    "        \n",
    "        # plot images and masks\n",
    "        ax[c, 2].imshow(pred_mask)\n",
    "        ax[c, 2].axis('off')\n",
    "        ax[c, 2].set_title('Predicted Mask')\n",
    "        \n",
    "        c+=1\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_code = df.loc[1]['Image']\n",
    "image = Image.open(os.path.join(TRAIN_PATH, str(id_code)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.size[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.size[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
