{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, absolute_import\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "# import data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# import image utils\n",
    "import PIL\n",
    "from PIL import Image, ImageOps, ImageEnhance\n",
    "\n",
    "# import image processing\n",
    "import scipy.ndimage as ndi\n",
    "import scipy\n",
    "\n",
    "# import image utilities\n",
    "from skimage.morphology import binary_opening, disk, label, binary_closing\n",
    "\n",
    "# import image augmentation\n",
    "from albumentations import (\n",
    "    HorizontalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,GaussianBlur,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, RandomBrightnessContrast, IAAPiecewiseAffine,\n",
    "    IAASharpen, IAAEmboss, Flip, OneOf, Compose, PadIfNeeded, RandomContrast, RandomGamma, RandomBrightness, ElasticTransform,\n",
    "    NoOp, RandomSizedCrop, RGBShift, VerticalFlip, RandomRotate90\n",
    ")\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, Sampler\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from torch.nn import init\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import math\n",
    "import os\n",
    "\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import cv2\n",
    "\n",
    "from skimage.morphology import binary_opening, disk, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = \"app.verta.ai\"\n",
    "\n",
    "PROJECT_NAME = \"Understanding_Clouds_from_Satellite_Images\"\n",
    "EXPERIMENT_NAME = \"VGGUNet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['VERTA_EMAIL'] = 'astakhova.aleksandra@gmail.com'\n",
    "os.environ['VERTA_DEV_KEY'] = 'd7ee32b5-bbd0-4c4c-a2ec-a070848021be'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set email from environment\n",
      "set developer key from environment\n",
      "connection successfully established\n",
      "set existing Project: Understanding_Clouds_from_Satellite_Images\n",
      "set existing Experiment: VGGUNet\n",
      "created new ExperimentRun: Run 1675715686598725244331\n"
     ]
    }
   ],
   "source": [
    "from verta import Client\n",
    "from verta.utils import ModelAPI\n",
    "\n",
    "client = Client(HOST)\n",
    "proj = client.set_project(PROJECT_NAME)\n",
    "expt = client.set_experiment(EXPERIMENT_NAME)\n",
    "run = client.set_experiment_run()\n",
    "\n",
    "run.log_tag('starter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = './train_images/'\n",
    "TEST_PATH = './test_images/'\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "#train_transforms = Compose([HorizontalFlip(p=0.5),\n",
    "#                            ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=30, p=0.2)])\n",
    "\n",
    "train_transforms = Compose([\n",
    "    OneOf([\n",
    "        ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1,\n",
    "                         rotate_limit=15,\n",
    "                         border_mode=cv2.BORDER_CONSTANT, value=0),\n",
    "        OpticalDistortion(distort_limit=0.11, shift_limit=0.15,\n",
    "                          border_mode=cv2.BORDER_CONSTANT,\n",
    "                          value=0),\n",
    "        ElasticTransform(alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n",
    "        NoOp()]),\n",
    "    RandomSizedCrop(min_max_height=(1050, 1400),\n",
    "                    height=1400,\n",
    "                    width=2100, p=0.3),\n",
    "    RandomRotate90(p=0.5),\n",
    "    HorizontalFlip(p=0.5),\n",
    "    VerticalFlip(p=0.5)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image_Label</th>\n",
       "      <th>EncodedPixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0011165.jpg_Fish</td>\n",
       "      <td>264918 937 266318 937 267718 937 269118 937 27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0011165.jpg_Flower</td>\n",
       "      <td>1355565 1002 1356965 1002 1358365 1002 1359765...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0011165.jpg_Gravel</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0011165.jpg_Sugar</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>002be4f.jpg_Fish</td>\n",
       "      <td>233813 878 235213 878 236613 878 238010 881 23...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Image_Label                                      EncodedPixels\n",
       "0    0011165.jpg_Fish  264918 937 266318 937 267718 937 269118 937 27...\n",
       "1  0011165.jpg_Flower  1355565 1002 1356965 1002 1358365 1002 1359765...\n",
       "2  0011165.jpg_Gravel                                                NaN\n",
       "3   0011165.jpg_Sugar                                                NaN\n",
       "4    002be4f.jpg_Fish  233813 878 235213 878 236613 878 238010 881 23..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image_Label</th>\n",
       "      <th>EncodedPixels</th>\n",
       "      <th>Image</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0011165.jpg_Fish</td>\n",
       "      <td>264918 937 266318 937 267718 937 269118 937 27...</td>\n",
       "      <td>0011165.jpg</td>\n",
       "      <td>Fish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0011165.jpg_Flower</td>\n",
       "      <td>1355565 1002 1356965 1002 1358365 1002 1359765...</td>\n",
       "      <td>0011165.jpg</td>\n",
       "      <td>Flower</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0011165.jpg_Gravel</td>\n",
       "      <td>-1</td>\n",
       "      <td>0011165.jpg</td>\n",
       "      <td>Gravel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0011165.jpg_Sugar</td>\n",
       "      <td>-1</td>\n",
       "      <td>0011165.jpg</td>\n",
       "      <td>Sugar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>002be4f.jpg_Fish</td>\n",
       "      <td>233813 878 235213 878 236613 878 238010 881 23...</td>\n",
       "      <td>002be4f.jpg</td>\n",
       "      <td>Fish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Image_Label                                      EncodedPixels  \\\n",
       "0    0011165.jpg_Fish  264918 937 266318 937 267718 937 269118 937 27...   \n",
       "1  0011165.jpg_Flower  1355565 1002 1356965 1002 1358365 1002 1359765...   \n",
       "2  0011165.jpg_Gravel                                                 -1   \n",
       "3   0011165.jpg_Sugar                                                 -1   \n",
       "4    002be4f.jpg_Fish  233813 878 235213 878 236613 878 238010 881 23...   \n",
       "\n",
       "         Image   Label  \n",
       "0  0011165.jpg    Fish  \n",
       "1  0011165.jpg  Flower  \n",
       "2  0011165.jpg  Gravel  \n",
       "3  0011165.jpg   Sugar  \n",
       "4  002be4f.jpg    Fish  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split column\n",
    "split_df = train[\"Image_Label\"].str.split(\"_\", n = 1, expand = True)\n",
    "# add new columns to train_df\n",
    "train['Image'] = split_df[0]\n",
    "train['Label'] = split_df[1]\n",
    "# fiil missing masks with '-1'\n",
    "train = train.fillna('-1')\n",
    "\n",
    "# check the result\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_to_mask(rle_string, height, width):\n",
    "    rows, cols = height, width\n",
    "    \n",
    "    if rle_string == -1:\n",
    "        return np.zeros((height, width))\n",
    "    else:\n",
    "        rle_numbers = [int(num_string) for num_string in rle_string.split(' ')]\n",
    "        rle_pairs = np.array(rle_numbers).reshape(-1,2)\n",
    "        img = np.zeros(rows*cols, dtype=np.uint8)\n",
    "        for index, length in rle_pairs:\n",
    "            index -= 1\n",
    "            img[index:index+length] = 255\n",
    "        img = img.reshape(cols,rows)\n",
    "        img = img.T\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CloudDataset(Dataset):\n",
    "    def __init__(self, dataset_path, df, transforms=None, size = (256, 256), mode = 'train'):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.df = df\n",
    "        self.transforms = transforms\n",
    "        self.size = size\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # open image and label\n",
    "        id_code = self.df.loc[idx]['Image']\n",
    "        image = Image.open(os.path.join(self.dataset_path, str(id_code)))\n",
    "        \n",
    "        if self.mode == 'train' or self.mode == 'test':\n",
    "            rle_mask = self.df.loc[idx]['EncodedPixels']\n",
    "            if rle_mask == '-1':\n",
    "                np_mask = np.zeros(self.size)\n",
    "            else:\n",
    "                np_mask = rle_to_mask(rle_mask, image.size[1], image.size[0])\n",
    "        else:\n",
    "            np_mask = np.zeros(self.size)\n",
    "        \n",
    "        # augment image and mask\n",
    "        if self.transforms != None:\n",
    "            if (rle_mask) == '-1':\n",
    "                augmented = self.transforms(image=np.array(image))\n",
    "            else:\n",
    "                augmented = self.transforms(image=np.array(image), mask = np_mask)\n",
    "                np_mask = augmented['mask']\n",
    "            image = Image.fromarray(augmented['image'], 'RGB')\n",
    "        \n",
    "        # normalize and convert to tensor\n",
    "        tf = transforms.Compose([transforms.Resize(self.size),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "        image = tf(image)\n",
    "        \n",
    "        if (rle_mask) != \"-1\":\n",
    "            tf = transforms.Compose([transforms.Resize(self.size),\n",
    "                                 transforms.ToTensor()])\n",
    "            mask = Image.fromarray(np_mask, 'L')\n",
    "            mask = tf(mask).float()\n",
    "        else:\n",
    "            mask = TF.to_tensor(np_mask).float()\n",
    "        \n",
    "        # in validation mode return image and id_code\n",
    "        if self.mode == 'validation':\n",
    "            return image, id_code\n",
    "\n",
    "        # return tensor with image and label\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCEDiceLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BCEDiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        bce = F.binary_cross_entropy_with_logits(input, target)\n",
    "        smooth = 1e-5\n",
    "        input = torch.sigmoid(input)\n",
    "        num = target.size(0)\n",
    "        input = input.view(num, -1)\n",
    "        target = target.view(num, -1)\n",
    "        intersection = (input * target)\n",
    "        dice = (2. * intersection.sum(1) + smooth) / (input.sum(1) + target.sum(1) + smooth)\n",
    "        dice = 1 - dice.sum() / num\n",
    "        return 0.5 * bce + dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(output, target):\n",
    "    smooth = 1e-5\n",
    "\n",
    "    output = torch.sigmoid(output).view(-1).data.cpu().numpy()\n",
    "    target = target.view(-1).data.cpu().numpy()\n",
    "    intersection = (output * target).sum()\n",
    "\n",
    "    return (2. * intersection + smooth) / \\\n",
    "        (output.sum() + target.sum() + smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_score(output, target):\n",
    "    smooth = 1e-5\n",
    "\n",
    "    if torch.is_tensor(output):\n",
    "        output = torch.sigmoid(output).data.cpu().numpy()\n",
    "    if torch.is_tensor(target):\n",
    "        target = target.data.cpu().numpy()\n",
    "    output_ = output > 0.5\n",
    "    target_ = target > 0.5\n",
    "    intersection = (output_ & target_).sum()\n",
    "    union = (output_ | target_).sum()\n",
    "\n",
    "    return (intersection + smooth) / (union + smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Source:\n",
    "https://github.com/ternaus/TernausNet\n",
    "'''\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models\n",
    "\n",
    "def conv3x3(in_, out):\n",
    "    return nn.Conv2d(in_, out, 3, padding=1)\n",
    "\n",
    "\n",
    "class ConvRelu(nn.Module):\n",
    "    def __init__(self, in_, out):\n",
    "        super().__init__()\n",
    "        self.conv = conv3x3(in_, out)\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            ConvRelu(in_channels, middle_channels),\n",
    "            nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class UNet11(nn.Module):\n",
    "    def __init__(self, num_filters=32, pretrained=False):\n",
    "        \"\"\"\n",
    "        :param num_classes:\n",
    "        :param num_filters:\n",
    "        :param pretrained:\n",
    "            False - no pre-trained network is used\n",
    "            True  - encoder is pre-trained with VGG11\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.encoder = models.vgg11(pretrained=pretrained).features\n",
    "\n",
    "        self.relu = self.encoder[1]\n",
    "        self.conv1 = self.encoder[0]\n",
    "        self.conv2 = self.encoder[3]\n",
    "        self.conv3s = self.encoder[6]\n",
    "        self.conv3 = self.encoder[8]\n",
    "        self.conv4s = self.encoder[11]\n",
    "        self.conv4 = self.encoder[13]\n",
    "        self.conv5s = self.encoder[16]\n",
    "        self.conv5 = self.encoder[18]\n",
    "\n",
    "        self.center = DecoderBlock(num_filters * 8 * 2, num_filters * 8 * 2, num_filters * 8)\n",
    "        self.dec5 = DecoderBlock(num_filters * (16 + 8), num_filters * 8 * 2, num_filters * 8)\n",
    "        self.dec4 = DecoderBlock(num_filters * (16 + 8), num_filters * 8 * 2, num_filters * 4)\n",
    "        self.dec3 = DecoderBlock(num_filters * (8 + 4), num_filters * 4 * 2, num_filters * 2)\n",
    "        self.dec2 = DecoderBlock(num_filters * (4 + 2), num_filters * 2 * 2, num_filters)\n",
    "        self.dec1 = ConvRelu(num_filters * (2 + 1), num_filters)\n",
    "\n",
    "        self.final = nn.Conv2d(num_filters, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.relu(self.conv1(x))\n",
    "        conv2 = self.relu(self.conv2(self.pool(conv1)))\n",
    "        conv3s = self.relu(self.conv3s(self.pool(conv2)))\n",
    "        conv3 = self.relu(self.conv3(conv3s))\n",
    "        conv4s = self.relu(self.conv4s(self.pool(conv3)))\n",
    "        conv4 = self.relu(self.conv4(conv4s))\n",
    "        conv5s = self.relu(self.conv5s(self.pool(conv4)))\n",
    "        conv5 = self.relu(self.conv5(conv5s))\n",
    "\n",
    "        center = self.center(self.pool(conv5))\n",
    "\n",
    "        dec5 = self.dec5(torch.cat([center, conv5], 1))\n",
    "        dec4 = self.dec4(torch.cat([dec5, conv4], 1))\n",
    "        dec3 = self.dec3(torch.cat([dec4, conv3], 1))\n",
    "        dec2 = self.dec2(torch.cat([dec3, conv2], 1))\n",
    "        dec1 = self.dec1(torch.cat([dec2, conv1], 1))\n",
    "        return self.final(dec1)\n",
    "\n",
    "\n",
    "def unet11(pretrained=False, **kwargs):\n",
    "    \"\"\"\n",
    "    pretrained:\n",
    "            False - no pre-trained network is used\n",
    "            True  - encoder is pre-trained with VGG11\n",
    "            carvana - all weights are pre-trained on\n",
    "                Kaggle: Carvana dataset https://www.kaggle.com/c/carvana-image-masking-challenge\n",
    "    \"\"\"\n",
    "    model = UNet11(pretrained=pretrained, **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "class Interpolate(nn.Module):\n",
    "    def __init__(self, size=None, scale_factor=None, mode='nearest', align_corners=False):\n",
    "        super(Interpolate, self).__init__()\n",
    "        self.interp = nn.functional.interpolate\n",
    "        self.size = size\n",
    "        self.mode = mode\n",
    "        self.scale_factor = scale_factor\n",
    "        self.align_corners = align_corners\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.interp(x, size=self.size, scale_factor=self.scale_factor,\n",
    "                        mode=self.mode, align_corners=self.align_corners)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderBlockV2(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channels, out_channels, is_deconv=True):\n",
    "        super(DecoderBlockV2, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        if is_deconv:\n",
    "            \"\"\"\n",
    "                Paramaters for Deconvolution were chosen to avoid artifacts, following\n",
    "                link https://distill.pub/2016/deconv-checkerboard/\n",
    "            \"\"\"\n",
    "\n",
    "            self.block = nn.Sequential(\n",
    "                ConvRelu(in_channels, middle_channels),\n",
    "                nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=4, stride=2,\n",
    "                                   padding=1),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            self.block = nn.Sequential(\n",
    "                Interpolate(scale_factor=2, mode='bilinear'),\n",
    "                ConvRelu(in_channels, middle_channels),\n",
    "                ConvRelu(middle_channels, out_channels),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class AlbuNet(nn.Module):\n",
    "    \"\"\"\n",
    "        UNet (https://arxiv.org/abs/1505.04597) with Resnet34(https://arxiv.org/abs/1512.03385) encoder\n",
    "        Proposed by Alexander Buslaev: https://www.linkedin.com/in/al-buslaev/\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=1, num_filters=32, pretrained=False, is_deconv=False):\n",
    "        \"\"\"\n",
    "        :param num_classes:\n",
    "        :param num_filters:\n",
    "        :param pretrained:\n",
    "            False - no pre-trained network is used\n",
    "            True  - encoder is pre-trained with resnet34\n",
    "        :is_deconv:\n",
    "            False: bilinear interpolation is used in decoder\n",
    "            True: deconvolution is used in decoder\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.encoder = models.resnet34(pretrained=pretrained)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv1 = nn.Sequential(self.encoder.conv1,\n",
    "                                   self.encoder.bn1,\n",
    "                                   self.encoder.relu,\n",
    "                                   self.pool)\n",
    "\n",
    "        self.conv2 = self.encoder.layer1\n",
    "\n",
    "        self.conv3 = self.encoder.layer2\n",
    "\n",
    "        self.conv4 = self.encoder.layer3\n",
    "\n",
    "        self.conv5 = self.encoder.layer4\n",
    "\n",
    "        self.center = DecoderBlockV2(512, num_filters * 8 * 2, num_filters * 8, is_deconv)\n",
    "\n",
    "        self.dec5 = DecoderBlockV2(512 + num_filters * 8, num_filters * 8 * 2, num_filters * 8, is_deconv)\n",
    "        self.dec4 = DecoderBlockV2(256 + num_filters * 8, num_filters * 8 * 2, num_filters * 8, is_deconv)\n",
    "        self.dec3 = DecoderBlockV2(128 + num_filters * 8, num_filters * 4 * 2, num_filters * 2, is_deconv)\n",
    "        self.dec2 = DecoderBlockV2(64 + num_filters * 2, num_filters * 2 * 2, num_filters * 2 * 2, is_deconv)\n",
    "        self.dec1 = DecoderBlockV2(num_filters * 2 * 2, num_filters * 2 * 2, num_filters, is_deconv)\n",
    "        self.dec0 = ConvRelu(num_filters, num_filters)\n",
    "        self.final = nn.Conv2d(num_filters, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.conv1(x)\n",
    "        conv2 = self.conv2(conv1)\n",
    "        conv3 = self.conv3(conv2)\n",
    "        conv4 = self.conv4(conv3)\n",
    "        conv5 = self.conv5(conv4)\n",
    "\n",
    "        center = self.center(self.pool(conv5))\n",
    "\n",
    "        dec5 = self.dec5(torch.cat([center, conv5], 1))\n",
    "\n",
    "        dec4 = self.dec4(torch.cat([dec5, conv4], 1))\n",
    "        dec3 = self.dec3(torch.cat([dec4, conv3], 1))\n",
    "        dec2 = self.dec2(torch.cat([dec3, conv2], 1))\n",
    "        dec1 = self.dec1(dec2)\n",
    "        dec0 = self.dec0(dec1)\n",
    "\n",
    "        if self.num_classes > 1:\n",
    "            x_out = F.log_softmax(self.final(dec0), dim=1)\n",
    "        else:\n",
    "            x_out = self.final(dec0)\n",
    "\n",
    "        return x_out\n",
    "\n",
    "\n",
    "class UNet16(nn.Module):\n",
    "    def __init__(self, num_classes=1, num_filters=32, pretrained=False, is_deconv=False):\n",
    "        \"\"\"\n",
    "        :param num_classes:\n",
    "        :param num_filters:\n",
    "        :param pretrained:\n",
    "            False - no pre-trained network used\n",
    "            True - encoder pre-trained with VGG16\n",
    "        :is_deconv:\n",
    "            False: bilinear interpolation is used in decoder\n",
    "            True: deconvolution is used in decoder\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.encoder = models.vgg16(pretrained=pretrained).features\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv1 = nn.Sequential(self.encoder[0],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[2],\n",
    "                                   self.relu)\n",
    "\n",
    "        self.conv2 = nn.Sequential(self.encoder[5],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[7],\n",
    "                                   self.relu)\n",
    "\n",
    "        self.conv3 = nn.Sequential(self.encoder[10],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[12],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[14],\n",
    "                                   self.relu)\n",
    "\n",
    "        self.conv4 = nn.Sequential(self.encoder[17],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[19],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[21],\n",
    "                                   self.relu)\n",
    "\n",
    "        self.conv5 = nn.Sequential(self.encoder[24],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[26],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[28],\n",
    "                                   self.relu)\n",
    "\n",
    "        self.center = DecoderBlockV2(512, num_filters * 8 * 2, num_filters * 8, is_deconv)\n",
    "\n",
    "        self.dec5 = DecoderBlockV2(512 + num_filters * 8, num_filters * 8 * 2, num_filters * 8, is_deconv)\n",
    "        self.dec4 = DecoderBlockV2(512 + num_filters * 8, num_filters * 8 * 2, num_filters * 8, is_deconv)\n",
    "        self.dec3 = DecoderBlockV2(256 + num_filters * 8, num_filters * 4 * 2, num_filters * 2, is_deconv)\n",
    "        self.dec2 = DecoderBlockV2(128 + num_filters * 2, num_filters * 2 * 2, num_filters, is_deconv)\n",
    "        self.dec1 = ConvRelu(64 + num_filters, num_filters)\n",
    "        self.final = nn.Conv2d(num_filters, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.conv1(x)\n",
    "        conv2 = self.conv2(self.pool(conv1))\n",
    "        conv3 = self.conv3(self.pool(conv2))\n",
    "        conv4 = self.conv4(self.pool(conv3))\n",
    "        conv5 = self.conv5(self.pool(conv4))\n",
    "\n",
    "        center = self.center(self.pool(conv5))\n",
    "\n",
    "        dec5 = self.dec5(torch.cat([center, conv5], 1))\n",
    "\n",
    "        dec4 = self.dec4(torch.cat([dec5, conv4], 1))\n",
    "        dec3 = self.dec3(torch.cat([dec4, conv3], 1))\n",
    "        dec2 = self.dec2(torch.cat([dec3, conv2], 1))\n",
    "        dec1 = self.dec1(torch.cat([dec2, conv1], 1))\n",
    "\n",
    "        if self.num_classes > 1:\n",
    "            x_out = F.log_softmax(self.final(dec1), dim=1)\n",
    "        else:\n",
    "            x_out = self.final(dec1)\n",
    "\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet16(pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "test_split = 0.2\n",
    "batch_size = 16\n",
    "epochs = 40\n",
    "learning_rate = 0.001\n",
    "num_workers = 1\n",
    "image_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_hyperparameter(\"test_split\", test_split)\n",
    "run.log_hyperparameter(\"batch_size\", batch_size)\n",
    "run.log_hyperparameter(\"epochs\", epochs)\n",
    "run.log_hyperparameter(\"learning_rate\", learning_rate)\n",
    "run.log_hyperparameter(\"image_size\", image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset and data loaders (for FISH)\n",
    "df = train[train['Label'] == 'Fish']\n",
    "df = df.set_index(pd.Series(range(len(df))))\n",
    "\n",
    "train_ds = CloudDataset(TRAIN_PATH, df, \n",
    "                        transforms=train_transforms, size = (image_size, image_size), mode = 'train')\n",
    "test_ds = CloudDataset(TRAIN_PATH, df, \n",
    "                       transforms=None, size = (image_size, image_size), mode = 'test')\n",
    "\n",
    "dataset_size = len(train_ds)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(test_split * dataset_size))\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "train_indices, test_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\n",
    "testloader = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, sampler=test_sampler, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "\n",
    "class RAdam(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        self.buffer = [[None, None, None] for ind in range(10)]\n",
    "        super(RAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "\n",
    "                state['step'] += 1\n",
    "                buffered = self.buffer[int(state['step'] % 10)]\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "\n",
    "                    # more conservative since it's an approximated value\n",
    "                    if N_sma >= 5:\n",
    "                        step_size = group['lr'] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = group['lr'] / (1 - beta1 ** state['step'])\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                if N_sma >= 5:            \n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n",
    "                else:\n",
    "                    p_data_fp32.add_(-step_size, exp_avg)\n",
    "\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = RAdam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_stats = pd.DataFrame(columns = ['Epoch', \n",
    "                                      'Time per epoch', \n",
    "                                      'Avg time per step', \n",
    "                                      'Train loss',\n",
    "                                      'DICE Train',\n",
    "                                      'Test loss',\n",
    "                                     'DICE Test']) \n",
    "criterion = BCEDiceLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_hyperparameter(\"optimizer\", \"RAdam\")\n",
    "run.log_hyperparameter(\"loss\", \"BCEDiceLoss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, device, trainloader, testloader, epochs, criterion, optimizer, train_stats):\n",
    "    #train the model\n",
    "    model.to(device)\n",
    "    \n",
    "    # learning rate scheduler\n",
    "    \n",
    "    #scheduler = lr_scheduler.CosineAnnealingLR(optimizer, len(trainloader), eta_min=0.000001)\n",
    "    #StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.15, patience=2)\n",
    "\n",
    "    steps = 0\n",
    "    running_loss = 0\n",
    "    dice_train = 0\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        since = time.time()\n",
    "\n",
    "        train_accuracy = 0\n",
    "        dice_train = 0\n",
    "        iou_train = 0\n",
    "        \n",
    "        for inputs, labels in tqdm_notebook(trainloader):\n",
    "            steps += 1\n",
    "            # Move input and label tensors to the default device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logps = model.forward(inputs)\n",
    "            \n",
    "            loss = criterion(logps, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            dice_train += dice_coef(logps, labels)\n",
    "            \n",
    "            iou_train += iou_score(logps, labels)\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "\n",
    "        test_loss = 0\n",
    "        dice_test = 0\n",
    "        iou_test = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in testloader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                logps = model.forward(inputs)\n",
    "                batch_loss = criterion(logps, labels)\n",
    "\n",
    "                test_loss += batch_loss.item()\n",
    "\n",
    "                # Calculate DICE\n",
    "                dice_test += dice_coef(logps, labels)\n",
    "                \n",
    "                iou_test += iou_score(logps, labels)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "              f\"Time per epoch: {time_elapsed:.4f}.. \"\n",
    "              f\"Average time per step: {time_elapsed/len(trainloader):.4f}.. \"\n",
    "              f\"Train loss: {running_loss/len(trainloader):.4f}.. \"\n",
    "              f\"Test loss: {test_loss/len(testloader):.4f}.. \"\n",
    "              f\"DICE Train: {dice_train/len(trainloader):.4f}.. \"\n",
    "              f\"DICE Test: {dice_test/len(testloader):.4f}.. \"\n",
    "              f\"IOU Train: {iou_train/len(trainloader):.4f}.. \"\n",
    "              f\"IOU Test: {iou_test/len(testloader):.4f}.. \"\n",
    "             )\n",
    "\n",
    "        train_stats = train_stats.append({'Epoch': epoch + 1, \n",
    "                                          'Time per epoch':time_elapsed, \n",
    "                                          'Avg time per step': time_elapsed/len(trainloader), \n",
    "                                          'Train loss' : running_loss/len(trainloader),\n",
    "                                          'Test loss' : test_loss/len(testloader),\n",
    "                                          'DICE Train' : dice_train/len(trainloader),\n",
    "                                          'DICE Test' : dice_test/len(testloader),\n",
    "                                         }, \n",
    "                                         ignore_index=True)\n",
    "\n",
    "        run.log_observation(\"time_per_epoch\", time_elapsed)\n",
    "        run.log_observation(\"time_per_step\", time_elapsed/len(trainloader))\n",
    "        run.log_observation(\"train_loss\", running_loss/len(trainloader))\n",
    "        run.log_observation(\"test_loss\", test_loss/len(testloader))\n",
    "        run.log_observation(\"dice_train\", dice_train/len(trainloader))\n",
    "        run.log_observation(\"dice_test\", dice_test/len(testloader))\n",
    "        run.log_observation(\"iou_train\", iou_train/len(trainloader))\n",
    "        run.log_observation(\"iou_test\", iou_test/len(testloader))\n",
    "\n",
    "        \n",
    "        scheduler.step(test_loss/len(testloader))\n",
    "        running_loss = 0\n",
    "        model.train()\n",
    "        \n",
    "    return model, train_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc1467eebdb74ecc9c9e5c82521b3a58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40.. Time per epoch: 627.3382.. Average time per step: 2.2566.. Train loss: 1.0049.. Test loss: 0.9525.. DICE Train: 0.3249.. DICE Test: 0.3735.. IOU Train: 0.2399.. IOU Test: 0.3151.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18d20054608147f394baf32a61945ae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/40.. Time per epoch: 618.8760.. Average time per step: 2.2262.. Train loss: 0.9848.. Test loss: 0.9278.. DICE Train: 0.3546.. DICE Test: 0.4230.. IOU Train: 0.2763.. IOU Test: 0.3473.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa9536429e7f43dc858b925fb19e61a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/40.. Time per epoch: 594.3718.. Average time per step: 2.1380.. Train loss: 0.9772.. Test loss: 0.9338.. DICE Train: 0.3672.. DICE Test: 0.4115.. IOU Train: 0.2896.. IOU Test: 0.3329.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6835401d8bf94904b3a705fc93d89035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/40.. Time per epoch: 600.8606.. Average time per step: 2.1614.. Train loss: 0.9725.. Test loss: 0.9387.. DICE Train: 0.3741.. DICE Test: 0.4255.. IOU Train: 0.2935.. IOU Test: 0.3334.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb8be443182427398c6a62085f39bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/40.. Time per epoch: 603.8538.. Average time per step: 2.1721.. Train loss: 0.9739.. Test loss: 0.9655.. DICE Train: 0.3712.. DICE Test: 0.3710.. IOU Train: 0.2890.. IOU Test: 0.3051.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a88fe5351e8c4bb4b7e3f0148fce596e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/40.. Time per epoch: 615.8859.. Average time per step: 2.2154.. Train loss: 0.9474.. Test loss: 0.9165.. DICE Train: 0.4053.. DICE Test: 0.4369.. IOU Train: 0.3230.. IOU Test: 0.3558.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca50894138c47359f63b4187873ab6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/40.. Time per epoch: 629.8675.. Average time per step: 2.2657.. Train loss: 0.9373.. Test loss: 0.9145.. DICE Train: 0.4221.. DICE Test: 0.4466.. IOU Train: 0.3369.. IOU Test: 0.3589.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a75043a42a64c25aa4732de09cabb00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/40.. Time per epoch: 617.6296.. Average time per step: 2.2217.. Train loss: 0.9315.. Test loss: 0.9169.. DICE Train: 0.4302.. DICE Test: 0.4517.. IOU Train: 0.3460.. IOU Test: 0.3623.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e73db0868a4875b0cbe36574765c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/40.. Time per epoch: 630.6848.. Average time per step: 2.2687.. Train loss: 0.9326.. Test loss: 0.9066.. DICE Train: 0.4294.. DICE Test: 0.4625.. IOU Train: 0.3434.. IOU Test: 0.3777.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d47ac3ad23d41b997fe5a49b99e9601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/40.. Time per epoch: 602.3729.. Average time per step: 2.1668.. Train loss: 0.9311.. Test loss: 0.9082.. DICE Train: 0.4326.. DICE Test: 0.4460.. IOU Train: 0.3478.. IOU Test: 0.3752.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c6e0e9afc0946938e2e10c2a09274f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/40.. Time per epoch: 615.4257.. Average time per step: 2.2138.. Train loss: 0.9281.. Test loss: 0.9045.. DICE Train: 0.4334.. DICE Test: 0.4510.. IOU Train: 0.3490.. IOU Test: 0.3724.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20721981dde744528be72669effdab71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/40.. Time per epoch: 626.2539.. Average time per step: 2.2527.. Train loss: 0.9273.. Test loss: 0.9045.. DICE Train: 0.4378.. DICE Test: 0.4772.. IOU Train: 0.3520.. IOU Test: 0.3794.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cf3e95cf0064de88e847298f03ac5d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/40.. Time per epoch: 618.3433.. Average time per step: 2.2243.. Train loss: 0.9271.. Test loss: 0.9130.. DICE Train: 0.4375.. DICE Test: 0.4528.. IOU Train: 0.3514.. IOU Test: 0.3654.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4294a35eb47c4b899485d1871404f16b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/40.. Time per epoch: 611.2770.. Average time per step: 2.1988.. Train loss: 0.9243.. Test loss: 0.9023.. DICE Train: 0.4404.. DICE Test: 0.4790.. IOU Train: 0.3548.. IOU Test: 0.3802.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7481a442a62b4bb2b53d986fe52ada81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/40.. Time per epoch: 609.7295.. Average time per step: 2.1933.. Train loss: 0.9238.. Test loss: 0.8981.. DICE Train: 0.4456.. DICE Test: 0.4639.. IOU Train: 0.3585.. IOU Test: 0.3800.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e63bdb8cc294b75b265ff21a48a62a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/40.. Time per epoch: 612.1536.. Average time per step: 2.2020.. Train loss: 0.9218.. Test loss: 0.8986.. DICE Train: 0.4490.. DICE Test: 0.4687.. IOU Train: 0.3616.. IOU Test: 0.3808.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc2150e6cde646baa1297952c0c1016e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/40.. Time per epoch: 599.3075.. Average time per step: 2.1558.. Train loss: 0.9214.. Test loss: 0.9110.. DICE Train: 0.4459.. DICE Test: 0.4588.. IOU Train: 0.3587.. IOU Test: 0.3705.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f8e66cb330d4a1b9397a7f93285bc81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/40.. Time per epoch: 635.4116.. Average time per step: 2.2857.. Train loss: 0.9209.. Test loss: 0.9038.. DICE Train: 0.4462.. DICE Test: 0.4524.. IOU Train: 0.3589.. IOU Test: 0.3755.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d15895dc5b74bb4868b8f61f9801f41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/40.. Time per epoch: 601.3573.. Average time per step: 2.1632.. Train loss: 0.9105.. Test loss: 0.8934.. DICE Train: 0.4589.. DICE Test: 0.4792.. IOU Train: 0.3733.. IOU Test: 0.3889.. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ca627be67b24eee88b97dc905b9a47e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train the model\n",
    "model, train_stats = train_model(model, device, trainloader, testloader, epochs, criterion, optimizer, train_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'vggunet_' + str(epochs) + '.pth'\n",
    "\n",
    "checkpoint = {'state_dict': model.state_dict()}\n",
    "\n",
    "torch.save(checkpoint, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save training stats\n",
    "train_stats.to_csv('vggunet_stats.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_dataset('train_stats', train_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_dataset('model', checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([obs[0] for obs in run.get_observation(\"test_loss\")], label=\"test\")\n",
    "plt.plot([obs[0] for obs in run.get_observation(\"train_loss\")], label=\"train\")\n",
    "plt.ylim(0, 1.4)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend(loc='best')\n",
    "run.log_image(\"loss\", plt)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([obs[0] for obs in run.get_observation(\"dice_test\")], label=\"test\")\n",
    "plt.plot([obs[0] for obs in run.get_observation(\"dice_train\")], label=\"train\")\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"dice\")\n",
    "plt.legend(loc='best')\n",
    "run.log_image(\"dice\", plt)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = 0\n",
    "dice_test = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in testloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logps = model.forward(inputs)\n",
    "        batch_loss = criterion(logps, labels)\n",
    "\n",
    "        test_loss += batch_loss.item()\n",
    "\n",
    "        # Calculate DICE\n",
    "        dice_test += dice_coef(logps, labels)\n",
    "        \n",
    "test_loss = test_loss / len(testloader)\n",
    "dice_test = dice_test / len(testloader)\n",
    "\n",
    "run.log_metric(\"test_loss\", test_loss)\n",
    "run.log_metric(\"dice_test\", dice_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask2rle(img):\n",
    "    '''\n",
    "    Convert mask to rle.\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels= img.T.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_width = image_size\n",
    "im_height = image_size\n",
    "\n",
    "orig_width = 2100\n",
    "orig_height = 1400\n",
    "\n",
    "threshold = 0.3\n",
    "\n",
    "n_images = 5\n",
    "fig, ax = plt.subplots(n_images, 3, figsize=(10, 10))\n",
    "\n",
    "indices = [np.random.choice(test_indices) for i in range(n_images)]\n",
    "\n",
    "model.eval()\n",
    "c = 0\n",
    "with torch.no_grad():\n",
    "    for idx in indices:\n",
    "        \n",
    "        id_code = df.loc[idx]['Image']\n",
    "        image = Image.open(os.path.join(TRAIN_PATH, str(id_code)))\n",
    "    \n",
    "        tf = transforms.Compose([transforms.Resize((im_width,im_height)),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "        inputs = tf(image)\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        output = model.forward(inputs.reshape(1,3,image_size,image_size))\n",
    "\n",
    "        pred_mask = torch.sigmoid(output.reshape(im_width,im_height)).data.cpu().numpy()\n",
    "        pred_mask = binary_opening(pred_mask > threshold, disk(2))\n",
    "\n",
    "        pred_mask = Image.fromarray((pred_mask * 255).astype(np.uint8)).resize((orig_width,orig_height))\n",
    "        \n",
    "        rle_mask = df.loc[idx]['EncodedPixels']\n",
    "        if rle_mask == '-1':\n",
    "            np_mask = np.zeros((orig_height, orig_width))\n",
    "        else:\n",
    "            np_mask = rle_to_mask(rle_mask, orig_height, orig_width)\n",
    "            \n",
    "        # plot images and masks\n",
    "        ax[c, 0].imshow(image)\n",
    "        ax[c, 0].axis('off')\n",
    "        ax[c, 0].set_title('Image')\n",
    "        \n",
    "        # plot images and masks\n",
    "        ax[c, 1].imshow(np_mask)\n",
    "        ax[c, 1].axis('off')\n",
    "        ax[c, 1].set_title('Original Mask')\n",
    "        \n",
    "        # plot images and masks\n",
    "        ax[c, 2].imshow(pred_mask)\n",
    "        ax[c, 2].axis('off')\n",
    "        ax[c, 2].set_title('Predicted Mask')\n",
    "        \n",
    "        c+=1\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_code = df.loc[1]['Image']\n",
    "image = Image.open(os.path.join(TRAIN_PATH, str(id_code)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.size[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.size[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
